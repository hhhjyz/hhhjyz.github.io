# 引言

<div align=center><img src="../assets/1%20引言/file-20250114155656232.png" width="400x"></div>


**机器学习：** 我们只需要定义一个灵活的程序算法，其输出由许多参数决定，然后使用数据集来确定当下的最佳参数集，这些参数通过某种性能度量方式来达到完成任务的最佳性能

在开始用机器学习算法解决问题之前，我们必须精确地定义问题，确定输入和输出的性质，并选择合适的模型族

每个数据集由一个个_样本_（example, sample）组成，大多时候，它们遵循独立同分布(independently and identically distributed, i.i.d.)。 样本有时也叫做 _数据点_ （data point）或者 _数据实例_（data instance），通常每个样本由一组称为 _特征_ （features，或 _协变量_ （covariates））的属性组成。 机器学习模型会根据这些属性进行预测。 在上面的监督学习问题中，要预测的是一个特殊的属性，它被称为 _标签_ （label，或 _目标_ （target））。

当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的 _维数_ （dimensionality）

 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为 _深度学习_ （deep learning）

**目标函数(Objective function)**:用于定义模型优劣程度的度量。我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为 _损失函数_（loss function，或cost function）

**数据集通常可以分成两部分：** 训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型

大多数流行的优化算法通常基于一种基本方法：梯度下降

## 1 各种机器学习问题

### 1.1 监督学习
**supervised learning:** 擅长在给定输入特征的情况下预测标签，每个“特征-标签”对都被称为一个样本

监督学习的学习过程一般可以分为三大步骤：

1. 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；
    
2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；
    
3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。

#### 1.1.1 回归 regression

当标签取一个任意数值时，我们称之为回归问题。

#### 1.1.2 分类 classification

分类问题希望模型能够预测样本属于哪个类别（class/category）

分类问题的常见损失函数被称为交叉熵（cross-entropy）

### 1.2 无监督学习

- 聚类问题：在没有标签的情况下给数据分类
- 主成分分析问题： 找到少量的参数来准确地描述捕捉数据的线性相关属性
- 因果关系和概率图模型问题： 描述观察到的许多数据的根本原因
- 生成对抗性网络

#### 1.2.1 与环境互动

不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为_离线学习_（offline learning）。

**强化学习**：在强化学习问题中，智能体在一系列的事件步骤上与环境交互，在每个特定的时间点，智能体从环境接受一些observation，并且必须选择一个action,然后通过某种机制将其传输回环境，最后智能体从环境中获得reward。

<div align=center><img src="../assets/1%20引言/file-20250114194224686.png" width="400x"></div>

一般的强化学习问题是一个非常普遍的问题。 智能体的动作会影响后续的观察，而奖励只与所选的动作相对应。 环境可以是完整观察到的，也可以是部分观察到的,解释所有这些复杂性可能会对研究人员要求太高。 此外，并不是每个实际问题都表现出所有这些复杂性。 因此，学者们研究了一些特殊情况下的强化学习问题。

当环境可被完全观察到时，强化学习问题被称为_马尔可夫决策过程_（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为 _上下文赌博机_ （contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的 _多臂赌博机_ （multi-armed bandit problem）。