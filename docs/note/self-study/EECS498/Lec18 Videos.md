!!! note 
	Video = 2D + Time

	视频就是一组图像组成的序列

	4D tensor： $T\times 2 \times H\times W$

!!! example "Video Classification"
	在视频中识别的是动作，人类正在执行的行为

_training on clips:_ 训练模型来识别低帧率的短的剪辑。在测试时在不同的剪辑上运行模型，对最后的预测取平均

## 1 Single-Frame CNN

最简单的 视频分类模型是**单帧 CNN(Single-Frame CNN)**, 只独立的对视频的各个帧进行分类, 忽略上下文信息(时间结构), 我们一般拿这个模型作为**基准(baseline)**, 其实已经足够好了

稍微复杂一点的模型叫做**后期融合(Late Fusion)**, 在单帧 CNN 的基础上, 把所有单帧特征融合并 展开为一个大向量， 然后在上面跑 MLP 分类

![](assets/Pasted%20image%2020250818153138.png)

后期融合可能会过拟合, 小改进是用全局池化 + 线性层来做分类  

后期融合的问题出在其难以学习帧期间的低像素级别的微小运动, 例如图中人物的抬脚

### 1.1 Early Fusion

![](assets/Pasted%20image%2020250818154953.png)

reshape四维张量，将时间维度解释为一组通道，然后融合所有的时间信息，使用一个2D-CNN，输出类分数

单层CNN的处理可能不够正确建模所有类型的时间交互

### 1.2 3D-CNN/slow fusion CNN

![](assets/Pasted%20image%2020250818160405.png)


### 1.3 Summary

![](assets/Pasted%20image%2020250818160821.png)

如果比较上述三种方法的架构和感受野, 会发现 Late Fusion 会在末尾突然扩张时间维度的感受野, Early Fusion 会在开头突然扩张时间维度的感受野, 3D CNN 则是慢慢扩张, 因此也称为 "Slow Fusion"

2D 卷积和 3D 卷积有明显不同, 在视频上的 2D 卷积没有**时移不变性**, 2D 卷积核在一小块空间以及整个时间轴上, 课程的例子是如果我们要学习图像颜色由橙到蓝的转变, 并且一个视频在多个时间点有这个转变, 我们的卷积核只能学习一个时间点上的转变, 即使这种转变是时移不变的, 对于每个时间点要重新弄一个卷积核  

对比而言, 3D 卷积核仅在一个空间和时间的小区域上, 然后进行滑动, 所以我们可以仅用单个卷积核识别由橙到蓝的转变

一个三维视觉中有名的模型是 [C3D](https://arxiv.org/pdf/1412.0767) 模型, 可以理解为 VGG + 3D CNN, 这个网络的启示是我们能不能让网络对运动更敏感, 因为人类能很容易从运动本身获取很多信息。不过计算量非常大

![](assets/Pasted%20image%2020250818162314.png)
## 2 Measuring Motion: Optical Flow

采用一组相邻的视频帧作为输入，然后计算一种流场，告诉我们每个每个像素在下一帧中将向什么方向移动：
$$
\begin{aligned}
&F(x,y) =(\mathrm d x, \mathrm d y)
\\
&I_{t+1}(x+\mathrm d x, y + \mathrm d y) = I_t(x,y)
\end{aligned}
$$

### 2.1 Separating Motion and Appearance: 双流网络

 它有两个并行独立的 CNN 栈, 一个处理空间, 仅对单帧预测分类; 另一个 CNN 仅处理运动信息, 通过 Early Fusion 将所有光流场处理分类, 最后我们取预测的平均值
![](assets/Pasted%20image%2020250818164400.png)

## 3 long-term temporal structure

上述方法都只能建模短期结构，建模长期结构需要用到RNN

对每个时间点使用CNN提取特征，通过某种RNN结构在时间上融合信息

我们可以建立一个多层卷积神经网络


### 3.1 Recurrent CNN

![](assets/Pasted%20image%2020250818172653.png)

这个二维网络中的每个点都是一个具有两个空间维度的三维张量，每个点上的张量将会结合同一个时间点上一层的特征和同一层上一个时间点的特征

但是计算效率比较低，RNN对长序列来说比较慢，并行度低


### 3.2 Spatio-Temporal Self-Attention(Nonlocal Block)

![](assets/Pasted%20image%2020250818174515.png)
我们先得到 3D 版本的自注意力层, 用 3D CNN 获得特征后算 , 得到注意力权重后再过一个 1x1x1 的 conv, 再加上残差链接后就得到了自注意力层, 在论文中叫做**非局部块(Nonlocal Block)**  

因为有残差链接, 这个非局部块可以插入 3D CNN 中微调应用, 非常有用, 最后架构大概是这样的
![](assets/Pasted%20image%2020250818174538.png)

下面介绍几种好用的 3D CNN 架构, 一个想法是将成熟的 2D CNN 架构**膨胀(inflating)** 为 3D CNN 架构  

将二维卷积改为三维卷积, 二维池化改为三维池化, 加一个额外的时间维就行了  

更进一步的想法是膨胀预训练权重, 将图像集上的训练权重在时间维度上复制后继续训练可以减少模型的收敛时间, 事实上这样的训练效果会更好

2019 年的 [SlowFast Network](https://arxiv.org/pdf/1812.03982) 取消了对外部光流的依赖, 仅对原始像素操作, 这个网络分为快慢两个分支, 慢分支以非常低的帧率运行, 但是使用大量通道; 快分支在高帧率上运行, 使用很少的通道, 同时有快分支指向慢分支的横向连接来融合信息, 最后用一个全连接层来预测

视频领域的另一个任务是**时空动作定位(Temporal Action Localization)** 不但识别出来多个动作, 还确定各个动作发生的时间跨度, 这个任务可以用类似 Faster R-CNN 的架构来解决  

更有挑战性的任务是**空间时间检测(Spatio-Temporal Detection)**, 这包括目标检测, 目标动作分类, 确定各个人各个动作发生的时间跨度

可视化视频模型和之前的可视化类似, 对于分类找到它的最大得分输出, 这部分可以见视频 67mins 处, 可以来通过输出猜猜对应的运动