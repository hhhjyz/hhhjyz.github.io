# 强化学习

## 1 强化学习问题定义

- 智能体agent: 智能体是强化学习算法的主体，它能够根据经验做出主观判断并执行动作，是整个智能系统的核心。
- 环境environment: 智能体以外的一切统称为环境，环境在与智能体的交互中，能被智能体所采取的动作影响，同时环境也能向智能体反馈状态和奖励。虽说智能体以外的一切都可视为环境，但在设计算法时常常会排除不相关的因素建立一个理想的环境模型来对算法功能进行模拟。
- 状态state: 状态可以理解为智能体对环境的一种理解和编码，通常包含了对智能体所采取决策产生影响的信息。
- 动作action
- 策略policy
- 奖励reword

强化学习实际上是智能体在与环境交互中去学习能帮助其获得最大化奖励这一策略的过
程。在每一次迭代中，智能体根据当前策略选择一个动作，该动作影响了环境，导致环
境发生了改变，智能体此时从环境得到状态变化和奖励反馈等信息，并根据这些反馈来
更新其内部的策略

- 基于评估
- 交互性：强化学习的数据在与环境的交互中产生
- 序列决策过程：智能主体在与环境的交互中需要作出⼀系列的决策，这些决策往往是前后关联的

![](assets/Chapter%206/file-20250508132617599.png)


!!! note 马尔可夫决策过程：机器人寻路问题
    ![](assets/Chapter%206/file-20250508133832532.png)
    - 主体：机器人
    - 环境：3* 3方格
    - 状态： 当前位置
    - 动作： 移动一格
    - 奖励：到达目标奖励，越界惩罚

**离散马尔可夫链**： 引入奖励机制
    ![](assets/Chapter%206/file-20250508134105586.png)

### 1.1 强化学习定义
- 策略函数$\pi$: $\pi(s,a)$状态s下做出a动作的概率。确定的策略函数：$a=\pi(s)$
- 价值函数$V$:$V_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$ ,智能体在时刻t处于状态s时，按照策略$\pi$采取行动获得的回报的期望
- 动作-价值函数$q$: $q_\pi(s,a)=\mathbb{E}_\pi[G-t|S_t = s, A_t = a]$
- 
